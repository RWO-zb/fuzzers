{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f103a2",
   "metadata": {},
   "source": [
    "##  Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4be63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import math\n",
    "from random import seed\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import impute\n",
    "import statistics\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import ceil\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import jaccard_score\n",
    "import time\n",
    "import multiprocessing\n",
    "from pymoo.algorithms.moo.nsga2 import calc_crowding_distance\n",
    "import subprocess\n",
    "import logging\n",
    "from csv import reader\n",
    "import argparse\n",
    "from stable_baselines3 import DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae02c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(arg.startswith('--f=') for arg in sys.argv):\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epsilon\", type=int, default=30)\n",
    "parser.add_argument(\"--abstract-level\", type=float, default=1)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f'epsilon: {args.epsilon}')\n",
    "DISPLAY_SCREEN = False\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "DD = args.abstract_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382437",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreAndTerminateWrapper(gym.Wrapper):\n",
    "    '''\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super(StoreAndTerminateWrapper,self).__init__(env)\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.env=env\n",
    "        self.mem = []\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_state = None\n",
    "        self.first_obs = 0\n",
    "        self.prev_obs = None\n",
    "        self.states_list = []\n",
    "        self.info = {}\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.current_step = 0\n",
    "        obs, info = self.env.reset(*args, **kwargs)\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_obs = obs\n",
    "        return obs,info\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step == 0:\n",
    "            self.prev_obs = self.first_obs\n",
    "            self.first_state = deepcopy(self.env)\n",
    "            self.states_list.append(self.first_state)\n",
    "        self.current_step += 1\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.TotalReward += reward\n",
    "        self.mem.append(tuple((self.prev_obs,action)))\n",
    "        self.prev_obs = obs\n",
    "        if self.current_step >= self.max_steps:\n",
    "          truncated = True\n",
    "        if obs[0] <= -1.2:\n",
    "          truncated = True\n",
    "          reward = -201 - self.TotalReward\n",
    "          self.TotalReward = -200\n",
    "        if terminated or truncated:\n",
    "          self.mem.append(tuple(('done',self.TotalReward)))\n",
    "        self.info['mem'] = self.mem\n",
    "        self.info['state'] = self.states_list\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.env = deepcopy(state)\n",
    "        obs = np.array(list(self.env.unwrapped.state))\n",
    "        self.current_step = 0\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_obs = obs\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fba8e",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4736946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def abstract_state(model, state1, d):\n",
    "    if type(state1) == str:\n",
    "        if state1 == 'done':\n",
    "            return 'end'\n",
    "    state_tensor = torch.as_tensor(state1).unsqueeze(0).to(model.device)\n",
    "    with torch.no_grad():  # 关闭梯度计算以提高效率\n",
    "        q_value = model.q_net(state_tensor).cpu().numpy()[0]  # 获取第一个样本的Q值\n",
    "    if q_value.ndim == 1:\n",
    "        return tuple(np.ceil(q_value / d))\n",
    "    else:\n",
    "        return [tuple(i) for i in np.ceil(q_value / d)]\n",
    "\n",
    "def predict(model, obs, deterministic=True):\n",
    "        obs = torch.as_tensor(obs).unsqueeze(0).to(model.device)\n",
    "        q_value = model.q_net(obs).cpu().detach().numpy()[0]\n",
    "        if deterministic:\n",
    "            return np.argmax(q_value)\n",
    "        else:\n",
    "            return np.random.choice([0, 1], p=q_value / q_value.sum())\n",
    "\n",
    "def action_probability(model, state):\n",
    "        state= torch.as_tensor(state).unsqueeze(0).to(model.device)\n",
    "        q_value = model.q_net(state).cpu().detach().numpy()[0]\n",
    "        if q_value.ndim == 1:\n",
    "            return q_value / q_value.sum()\n",
    "        else:\n",
    "            div_factor = q_value.sum(axis=1)\n",
    "            return (q_value.T / div_factor).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874c7fa",
   "metadata": {},
   "source": [
    "## RL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportional_sampling_whitout_replacement(index, size):\n",
    "    s = 0\n",
    "    s = sum(np.array(index))\n",
    "    p = [ind / s for ind in index]\n",
    "    samples = np.random.choice(index, size=size, replace=False, p=p)\n",
    "    return samples\n",
    "\n",
    "def population_sample(episodes , ind,  pop_size , threshold, functional_fault_size, reward_fault_size):\n",
    "  \"\"\"\n",
    "  This function is meant to sample episodes from training after that you need to add test episodes using random_test \n",
    "  Set the parameters as you want but be careful the input episodes for this function is the memory of the agent and each step has seperate index \n",
    "  this function returns the final steps of the selected function then you need to extract that episodes from the input memore that is called 'episodes'\n",
    "  use the episodes extract function ... \n",
    "\n",
    "  samples n episodes from training n1 functinal faults and n2 reward faults \n",
    "  reward faults are episodes with reward bellow the thresthreshold \n",
    "  from random test samples M episodes m1 random episode and\n",
    "  m2 episodes with sudden reward change we dont have a sudden reward change in this example  \n",
    "  \"\"\"\n",
    "  epsilon = 0.1\n",
    "  index = []\n",
    "  functional_fault = []\n",
    "  reward_fault = []\n",
    "  start_states =[]\n",
    "  ind  = np.where(np.array(episodes)==('done',))\n",
    "  index= ind[0]\n",
    "  print(len(ind[0]),'episodes from training')\n",
    "  population=[]\n",
    "  for i in index:\n",
    "    _,r = episodes[i]\n",
    "    if abs(episodes[i-1][0][0])<(mtc_wrapped.low[0]+epsilon):\n",
    "      functional_fault.append(i)\n",
    "      print('function fault') \n",
    "    if r<threshold:\n",
    "      reward_fault.append(i)\n",
    "      print('reward fault')\n",
    "  if len(functional_fault)<functional_fault_size:\n",
    "    print('functional faults size is' ,len(functional_fault),' and its less than desired number' )\n",
    "    population += functional_fault\n",
    "    print('sampling more random episodes instead ...!')\n",
    "  if len(functional_fault)==functional_fault_size:\n",
    "    population += functional_fault\n",
    "  if len(functional_fault)>functional_fault_size:\n",
    "    # proportianl_sample_whitout_replacement()\n",
    "    sam1=proportional_sampling_whitout_replacement(functional_fault,functional_fault_size)\n",
    "    print(population)\n",
    "    print(\"ff\",len(functional_fault))\n",
    "    population += sam1\n",
    "  if len(reward_fault)<reward_fault_size:\n",
    "    print('reward faults size is' ,len(reward_fault),' and its less than desired number' )\n",
    "    population += reward_fault\n",
    "    print('sampling more random episodes instead ...!')\n",
    "  if len(reward_fault)==reward_fault_size:\n",
    "    population += reward_fault\n",
    "  if len(reward_fault)>reward_fault_size:\n",
    "    #proportional sampling\n",
    "    sam2 = proportional_sampling_whitout_replacement(reward_fault,reward_fault_size)\n",
    "    population += list(sam2)\n",
    "  r_size= pop_size-len(population)\n",
    "  # random_test(model,env,r_size)\n",
    "  print(\"RF\",len(reward_fault))\n",
    "  # population += reward_fault\n",
    "  return population , r_size\n",
    "\n",
    "def episode_extract(sampled_index, episodes):\n",
    "  epis = []\n",
    "  for i in sampled_index:\n",
    "    # print(episodes[i])\n",
    "    j = i-1\n",
    "    while not episodes[j][0] == 'done':\n",
    "      # print(episodes[j])\n",
    "      if j==0:\n",
    "        break\n",
    "      j-=1\n",
    "    slice1 = episodes[(j+1):(i+1)]\n",
    "    epis.append(slice1)\n",
    "    assert len(slice1)>0, 'Attempt to return Empty episode'\n",
    "  return epis\n",
    "\n",
    "def fitness_reward(episode):\n",
    "    \"\"\"\n",
    "    here the reward could be calculated as the lengh of the episode; Since the\n",
    "    reward of the cartpole is defined based on the number of steps without falling\n",
    "    last part of the episode contains the signal of ('done',reward)\n",
    "    \"\"\"\n",
    "    return len(episode) - 1\n",
    "\n",
    "def action_probability(model, state):\n",
    "        state= torch.as_tensor(state).unsqueeze(0).to(model.device)\n",
    "        q_value = model.q_net(state).cpu().detach().numpy()[0]\n",
    "        if q_value.ndim == 1:\n",
    "            return q_value / q_value.sum()\n",
    "        else:\n",
    "            div_factor = q_value.sum(axis=1)\n",
    "            return (q_value.T / div_factor).T\n",
    "\n",
    "def fitness_confidence(episode, model, mode):\n",
    "  \"\"\"\n",
    "  confidence level is define as differences between the highest and\n",
    "  second highest action probabilities of selecting actions OR\n",
    "  the ratio between the highest and lowest/second highest action probability\n",
    "  :param `mode`: r for ration and m for differences \n",
    "  :param `model`: is the RL agent \n",
    "  :param `episode`: is the episode values or sequence from the rl \n",
    "  \"\"\"\n",
    "  cl = 0.0\n",
    "  for i in range(len(episode)):\n",
    "    if i==(len(episode)-1):\n",
    "        if episode[i][0]=='done':\n",
    "            return (cl/(len(episode)-1))\n",
    "        else:\n",
    "            assert False, \"last state is not done , reward\"\n",
    "    else:\n",
    "      prob=action_probability(model,episode[i][0])\n",
    "      high1=prob.argmax()\n",
    "      first = prob[high1]\n",
    "      temp = prob\n",
    "      temp[high1] = 0.0\n",
    "      high2= temp.argmax()\n",
    "      second = prob[high2]\n",
    "      if mode == 'r':\n",
    "        cl +=  (first/second)\n",
    "        #In the next version this will be updated to a normalized ratio to avoid having large values \n",
    "      if mode == 'm':\n",
    "        cl += (first - second) #To_Do: first - second / first +second this one is better \n",
    "  print(\"WARNING nothing returned\", episode )\n",
    "\n",
    "def fitness_reward_probability(ml, binary_episode):\n",
    "    \"\"\"\n",
    "    This function returns the third fitness funciton that is ment to guide the search toward\n",
    "    the episodes with a higher probability of a reward fault and as we have a minimizing\n",
    "    optimization funciton in MOSA we neeed to change this functionwe can either go with the\n",
    "    negation of the probability of the reward fault = 1-probability of the reward fault\n",
    "    that is equal to the probability of the bein a non-faulty episode\n",
    "    :param `ml`: RF_FF_1rep for functional fault\n",
    "    :param `binary episode`: episodes decodeed as having abstract states\n",
    "    \"\"\"\n",
    "    # return -(ml.predict_proba(episode)[0][1])\n",
    "    return ml.predict_proba(binary_episode)[0][0]\n",
    "\n",
    "def fitness_functional_probability(ml, binary_episode):\n",
    "    return ml.predict_proba(binary_episode)[0][0]\n",
    "\n",
    "def abstract_state(model, state1, d):\n",
    "    if type(state1) == str:\n",
    "        if state1 == 'done':\n",
    "            return 'end'\n",
    "    state_tensor = torch.as_tensor(state1).unsqueeze(0).to(model.device)\n",
    "    with torch.no_grad():  # 关闭梯度计算以提高效率\n",
    "        q_values = model.q_net(state_tensor).cpu().numpy()[0]  # 获取第一个样本的Q值\n",
    "    return tuple([ceil(q_value/d) for q_value in q_values])\n",
    "\n",
    "def report(model2, x_train, y_train, x_test, y_test):\n",
    "    plt.ion()\n",
    "    print(\"********************** reporting the result of the model **************************\")\n",
    "    print('The score for train data is {0}'.format(model2.score(x_train, y_train)))\n",
    "    print('The score for test data is {0}'.format(model2.score(x_test, y_test)))\n",
    "\n",
    "    predictions_train = model2.predict(x_train)\n",
    "    predictions_test = model2.predict(x_test)\n",
    "\n",
    "    print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
    "\n",
    "    print(\n",
    "        'the test recall for the class yes is {0}'.format(metrics.recall_score(y_test, predictions_test, pos_label=1)))\n",
    "    print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test, predictions_test, pos_label=0)))\n",
    "\n",
    "    print('the training recall for the class yes is {0}'.format(\n",
    "        metrics.recall_score(y_train, predictions_train, pos_label=1)))\n",
    "    print('the training recall for the class no is {0}'.format(\n",
    "        metrics.recall_score(y_train, predictions_train, pos_label=0)))\n",
    "\n",
    "    print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
    "\n",
    "    print('the test precision for the class yes is {0}'.format(\n",
    "        metrics.precision_score(y_test, predictions_test, pos_label=1)))\n",
    "    print('the test precision for the class no is {0}'.format(\n",
    "        metrics.precision_score(y_test, predictions_test, pos_label=0)))\n",
    "\n",
    "    print('the training precision for the class yes is {0}'.format(\n",
    "        metrics.precision_score(y_train, predictions_train, pos_label=1)))\n",
    "    print('the training precision for the class no is {0}'.format(\n",
    "        metrics.precision_score(y_train, predictions_train, pos_label=0)))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(classification_report(y_test, predictions_test, target_names=['NO ', 'yes']))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"\\n\\nspecifity :\", specificity)\n",
    "    print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
    "    CM = metrics.confusion_matrix(y_test, predictions_test)\n",
    "    print(\"The confusion Matrix:\")\n",
    "    print(CM)\n",
    "    print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
    "    print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
    "    ConfusionMatrixDisplay(CM, display_labels=model2.classes_).plot()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')\n",
    "    display.plot()\n",
    "    plt.pause(3)\n",
    "    plt.ioff()\n",
    "\n",
    "def fix_testing(testing_episodes, testing_states, Env2):\n",
    "    buffer = []\n",
    "    episodes_set = []\n",
    "    j = 0\n",
    "    for i in range(len(testing_episodes)):\n",
    "        if type(testing_episodes[i][0]) is str and testing_episodes[i][0] == 'done':\n",
    "            if i == 0:\n",
    "                continue\n",
    "            buffer.append(testing_episodes[i])\n",
    "            episodes_set.append(buffer)\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(testing_episodes[i])\n",
    "    if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "        del testing_states[0]\n",
    "    if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "        assert False, 'problem in starting states'\n",
    "    if len(episodes_set) != len(testing_states):\n",
    "        del testing_states[-1]\n",
    "    if len(episodes_set) != len(testing_states):\n",
    "        assert False, 'problem in data prepration'\n",
    "    return episodes_set, testing_states\n",
    "\n",
    "def is_functional_fault(episode):\n",
    "    epsilon = 0.1\n",
    "    env = mtc_wrapped\n",
    "    \n",
    "    # 检查 episode 是否以 'done' 结束\n",
    "    if episode[-1][0] == 'done':\n",
    "        # 获取倒数第二个状态（实际的最后一个状态）\n",
    "        if len(episode) < 2:\n",
    "            return False\n",
    "        last_state = episode[-2][0]\n",
    "        reward = episode[-1][1]  # 奖励在 'done' 元组中\n",
    "    else:\n",
    "        last_state = episode[-1][0]\n",
    "        reward = episode[-1][1] if len(episode[-1]) > 1 else 0\n",
    "    \n",
    "    # 确保 last_state 是数值类型\n",
    "    if isinstance(last_state, (int, float, np.number)):\n",
    "        # 功能故障的条件：位置过低且奖励为 -200\n",
    "        position_too_low = last_state < (env.low[0] + epsilon)\n",
    "        has_low_reward = reward == -200\n",
    "        \n",
    "        return position_too_low and has_low_reward\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_reward_fault(episode):\n",
    "    RF_threshold = -180\n",
    "    \n",
    "    # 获取奖励\n",
    "    if episode[-1][0] == 'done':\n",
    "        reward = episode[-1][1]\n",
    "    else:\n",
    "        reward = episode[-1][1] if len(episode[-1]) > 1 else 0\n",
    "    \n",
    "    # 奖励故障的条件：奖励低于阈值且 episode 长度超过 200\n",
    "    return reward < RF_threshold and len(episode) > 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66581513",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Abstract_classes(ep,abstraction_d,model):\n",
    "  d=abstraction_d\n",
    "  abs_states1=[]\n",
    "  for episode in ep:\n",
    "    for state,action in episode:\n",
    "      abs_st = abstract_state(model,state,d)\n",
    "      if abs_st == 'end':\n",
    "        continue\n",
    "      abs_states1.append(abs_st)\n",
    "  unique1=list(set(abs_states1))\n",
    "  uni1 = np.array(unique1)\n",
    "  a=len(abs_states1)\n",
    "  b=len(set(abs_states1))\n",
    "  print(\"abstract states:\",b)\n",
    "  print(\"Concrete states\",a)\n",
    "  print(\"ratio\",b/a)\n",
    "  return unique1,uni1\n",
    "#need modify\n",
    "def ML_first_representation_func_based(Abs_d,functional_func,reward_func,model,input_episodes,unique1):\n",
    "  \"\"\"\n",
    "  TO-DO : fix epsilon and threshold\n",
    "  \"\"\"\n",
    "  d = Abs_d\n",
    "  data1_x_b=[]\n",
    "  data1_y_b= [] \n",
    "  data1_y_f_b = []\n",
    "  for i, episode in enumerate(input_episodes):\n",
    "    record = np.zeros(len(unique1))\n",
    "    temp_flag = False\n",
    "    for state, action in episode:\n",
    "      ab = abstract_state(model,state,d)\n",
    "      if ab == 'end':\n",
    "        assert not temp_flag, f'Episode data problem, two terminations in one episode. Episode number{i}'\n",
    "        temp_flag = True\n",
    "        # print(action)\n",
    "        # print(functional_func(episode))\n",
    "        if functional_func(episode):\n",
    "          data1_y_f_b.append(1)\n",
    "        else:\n",
    "          data1_y_f_b.append(0)\n",
    "        if reward_func(episode):\n",
    "          data1_y_b.append(1)\n",
    "        else:\n",
    "          data1_y_b.append(0)\n",
    "        # print(\"end\\n\\n\\n\")\n",
    "        # print(len(data1_y_b),\"len(input_episodes)\",len(input_episodes))\n",
    "        continue\n",
    "        # print(state[0])\n",
    "      ind = unique1.index(ab)\n",
    "      record[ind] = 1\n",
    "      # print(state, action)\n",
    "      assert len(data1_y_b)<len(input_episodes), \"assert\"\n",
    "      # if you want the frequency go with the next line \n",
    "      # record[ind] += 1\n",
    "    data1_x_b.append(record)\n",
    "\n",
    "  return data1_x_b, data1_y_b, data1_y_f_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e588b9",
   "metadata": {},
   "source": [
    "## Genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(episode,model, d, unique5):\n",
    "  \"\"\"\n",
    "  thid function takes the concrete episodes and returns the encoded episodes \n",
    "  based on the presence and absence of the individuals  \n",
    "  :param 'episode': input episode\n",
    "  :param 'model': RL model\n",
    "  :param 'd': abstraction level = 1\n",
    "  :param 'unique5': abstract classes \n",
    "  :return: encoded episodse based on the presence and absence\n",
    "\n",
    "  \"\"\"\n",
    "  d=d\n",
    "  record = np.zeros(len(unique5))\n",
    "  for state, action in episode:\n",
    "    ab = abstract_state(model,state,d)\n",
    "    if ab == 'end':\n",
    "      continue\n",
    "    if ab in unique5:\n",
    "      ind = unique5.index(ab)\n",
    "    record[ind] = 1\n",
    "  return [record]\n",
    "\n",
    "def transform(state):\n",
    "  position = state[0]\n",
    "  noise = np.random.uniform(low=0.95, high=1.05)\n",
    "  new_position= position * noise \n",
    "  new_state =deepcopy(state)\n",
    "  new_state[0] = new_position \n",
    "  return new_state\n",
    "\n",
    "def mutation_improved(population, model, env, objective_uncovered):\n",
    "    \"\"\"\n",
    "    This is the final mutation function\n",
    "    It takes the population as input and returns the mutated individual\n",
    "    :param 'population': Population that we want to mutate\n",
    "    :param 'model': RL model\n",
    "    :param 'env': RL environment\n",
    "    :param 'objective_uncovered: uncovered ubjectives for tournament selection\n",
    "    :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
    "    To-do:\n",
    "    move deepcopy to the cadidate class methods .set info\n",
    "    \"\"\"\n",
    "    parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "    parent1 = deepcopy(parent.get_candidate_values())\n",
    "    if len(parent1) < 3:\n",
    "        assert False, \"parent in mutation is shorter than 3\"\n",
    "    Mutpoint = random.randint(3, (len(parent1) - 3))\n",
    "    new_state = transform(parent1[Mutpoint][0])\n",
    "    action = model.predict(new_state)\n",
    "    if action != int(parent1[Mutpoint][1]):\n",
    "        print('Mutation lured the agent ... ')\n",
    "    new_parent = parent1[:Mutpoint]\n",
    "    new_parent.append([new_state, 'Mut'])\n",
    "    new_cand = Candidate(new_parent)\n",
    "    new_cand.set_start_state(parent.get_start_state())\n",
    "\n",
    "    re_executed_epis = re_execute(model, env, new_cand)\n",
    "\n",
    "    re_executed_cand = Candidate(re_executed_epis)\n",
    "    re_executed_cand.set_start_state(new_cand.get_start_state())\n",
    "    re_executed_cand.set_info(deepcopy(parent.get_info()))\n",
    "    re_executed_cand.set_info([\"mutation is done! \", \"mutpoint was:\", Mutpoint])\n",
    "\n",
    "    return re_executed_cand\n",
    "#need modify\n",
    "def mutation_improved_p(parent, model, env, m_rate):\n",
    "    \"\"\"\n",
    "    This is the final mutation function with input of a parent considering internal m_rate\n",
    "    Here we give the parent to themutation funcion based on the given mutation\n",
    "    rate of m_rate, we may mutate the episodes.\n",
    "    :param 'parent' : individual that we want to mutate\n",
    "    :param 'model': RL model\n",
    "    :param 'env': RL environment\n",
    "    :param 'm_rate': mutation : recommended value is 1/len(parent)\n",
    "    :return : mutated individual\n",
    "    To-do:\n",
    "    move deepcopy to the cadidate .set info\n",
    "    \"\"\"\n",
    "    # parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "    global MUTATION_NUMBER\n",
    "    chance = random.uniform(0, 1)\n",
    "    if chance > m_rate:\n",
    "        return parent\n",
    "    else:\n",
    "        parent1 = deepcopy(parent.get_candidate_values())\n",
    "        if len(parent1) < 3:\n",
    "            assert False, \"parent in mutation is shorter than 3\"\n",
    "        Mutpoint = random.randint(1, (len(parent1) - 3))\n",
    "        new_state = transform(parent1[Mutpoint][0])\n",
    "        action = model.predict(new_state)\n",
    "        if action != int(parent1[Mutpoint][1]):\n",
    "            print('Mutation lured the agent ... ')\n",
    "        new_parent = parent1[:Mutpoint]\n",
    "        new_parent.append([new_state, 'Mut'])\n",
    "        new_cand = Candidate(new_parent)\n",
    "        new_cand.set_start_state(parent.get_start_state())\n",
    "        re_executed_epis = re_execute(model, env, new_cand)\n",
    "        re_executed_cand = Candidate(re_executed_epis)\n",
    "        re_executed_cand.set_start_state(new_cand.get_start_state())\n",
    "        re_executed_cand.set_info(deepcopy(parent.get_info()))\n",
    "        re_executed_cand.set_info([\"mutation is done! \", \"mutpoint was:\", Mutpoint])\n",
    "        MUTATION_NUMBER += 1\n",
    "        return re_executed_cand\n",
    "    \n",
    "#need modify    \n",
    "def Crossover_improved_v2(population, model, d, objective_uncovered):\n",
    "    \"\"\"\n",
    "    This is the crossover function that we are using\n",
    "    It takes the population as input and returns the mutated individual\n",
    "    :param 'population': Population. we select a parent based on the tournament\n",
    "     selection and then select the mutation point and then search for the matching point.\n",
    "    :param 'model': RL model\n",
    "    :param 'd': abstraction level\n",
    "    :param 'objective_uncovered: uncovered objectives for tournament selection\n",
    "    :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
    "    \"\"\"\n",
    "    found_match = False\n",
    "    matches_list = []\n",
    "    \n",
    "    while not found_match:\n",
    "        parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "        parent1 = deepcopy(parent.get_candidate_values())\n",
    "        parent1_start_point = deepcopy(parent.get_start_state())\n",
    "        \n",
    "        if len(parent1) < 6:\n",
    "            continue\n",
    "            # assert False, 'input of crossover is shorter than expected '\n",
    "        \n",
    "        crosspoint = random.randint(2, (len(parent1) - 3))\n",
    "        abs_class = abstract_state(model, parent1[crosspoint][0], d)  # 单个状态\n",
    "        \n",
    "        for i in range(50):\n",
    "            indx = random.randint(0, len(population) - 1)\n",
    "            random_candidate = deepcopy(population[indx])\n",
    "            random_cand_data = random_candidate.get_candidate_values()\n",
    "            \n",
    "            if len(random_cand_data) < 8:\n",
    "                continue\n",
    "                \n",
    "            random_cand_start_point = random_candidate.get_start_state()\n",
    "            \n",
    "            # 逐个状态检查匹配，而不是传递整个列表\n",
    "            for st_index in range(2, len(random_cand_data) - 3):\n",
    "                try:\n",
    "                    random_ab = abstract_state(model, random_cand_data[st_index][0], d)  # 单个状态\n",
    "                    \n",
    "                    # 检查抽象状态是否匹配\n",
    "                    if isinstance(random_ab, (list, tuple)) and isinstance(abs_class, (list, tuple)):\n",
    "                        if len(random_ab) == len(abs_class) and all(r == a for r, a in zip(random_ab, abs_class)):\n",
    "                            matches_list.append(st_index)\n",
    "                            found_match = True\n",
    "                    elif random_ab == abs_class:\n",
    "                        matches_list.append(st_index)\n",
    "                        found_match = True\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in abstract_state at index {st_index}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            if found_match and matches_list:\n",
    "                break\n",
    "                \n",
    "        if found_match and matches_list:\n",
    "            break\n",
    "\n",
    "    if not matches_list:\n",
    "        # 如果没有找到匹配，返回原始父代或进行其他处理\n",
    "        print(\"No match found in crossover, returning original parent\")\n",
    "        return parent, parent\n",
    "\n",
    "    index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
    "    matchpoint = matches_list[index_match_in_matchlist]\n",
    "    match_candidate = deepcopy(random_candidate)\n",
    "    match = deepcopy(random_cand_data)\n",
    "    match_start = deepcopy(random_cand_start_point)\n",
    "    \n",
    "    offspring1 = deepcopy(parent1[:crosspoint])\n",
    "    offspring1 += deepcopy(match[matchpoint:])\n",
    "    offspring1[-1] = ['done', (len(offspring1) - 1)]\n",
    "    candid1 = Candidate(offspring1)\n",
    "    candid1.set_start_state(parent1_start_point)\n",
    "    candid1.set_info(deepcopy(parent.get_info()))\n",
    "    candid1.set_info([\"crossover is Done!\", \"the crossover point is:\", crosspoint])\n",
    "    \n",
    "    offspring2 = deepcopy(match[:matchpoint])\n",
    "    offspring2 += deepcopy(parent1[crosspoint:])\n",
    "    offspring2[-1] = ['done', (len(offspring2) - 1)]\n",
    "    candid2 = Candidate(offspring2)\n",
    "    candid2.set_start_state(match_start)\n",
    "    candid2.set_info(deepcopy(match_candidate.get_info()))\n",
    "    candid2.set_info([\"crossover is Done!\", \"the crossover point is:\", matchpoint])\n",
    "\n",
    "    if len(offspring1) < 3:\n",
    "        print(offspring1)\n",
    "        assert False, 'created offspring 1 in crossover is shorter than expected '\n",
    "\n",
    "    if len(offspring2) < 3:\n",
    "        print(offspring2)\n",
    "        assert False, 'created offspring 2 in crossover is shorter than expected '\n",
    "\n",
    "    return candid1, candid2\n",
    "#need to modify\n",
    "def Crossover_improved_v2_random(population, model, d, objective_uncovered):\n",
    "    found_match = False\n",
    "    while not found_match:\n",
    "        i = random.randint(0, len(population))\n",
    "        parent1 = deepcopy(population[i].get_candidate_values())\n",
    "        parent1_start_point = deepcopy(population[i].get_start_state())\n",
    "        matches_list = []\n",
    "        crosspoint = random.randint(1, (len(parent1) - 3))\n",
    "        abs_class = list(model.abstract_state(parent1[crosspoint][0], d))\n",
    "        attemp = 0\n",
    "        for i in range(700):\n",
    "            attemp += 1\n",
    "            indx = random.randint(0, len(population) - 1)\n",
    "            random_candidate = deepcopy(population[indx])\n",
    "            random_cand_data = random_candidate.get_candidate_values()\n",
    "            random_cand_start_point = random_candidate.get_start_state()\n",
    "            for st_index in range(1, len(random_cand_data) - 3):\n",
    "                random_ab = list(model.abstract_state(random_cand_data[st_index][0], d))\n",
    "                if random_ab == abs_class:\n",
    "                    matches_list.append(st_index)\n",
    "                    found_match = True\n",
    "            if found_match:\n",
    "                break\n",
    "    print(\"match found in --- attemps\", attemp)\n",
    "    index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
    "    matchpoint = matches_list[index_match_in_matchlist]\n",
    "    match_candidate = random_candidate\n",
    "    match = random_cand_data\n",
    "    match_start = deepcopy(random_cand_start_point)\n",
    "    offspring1 = deepcopy(parent1[:crosspoint])\n",
    "    offspring1 += deepcopy(match[matchpoint:])\n",
    "    offspring1[-1] = ['done', (len(offspring1) - 1)]\n",
    "    candid1 = Candidate(offspring1)\n",
    "    candid1.set_start_state(parent1_start_point)\n",
    "\n",
    "    offspring2 = deepcopy(match[:matchpoint])\n",
    "    offspring2 += deepcopy(parent1[crosspoint:])\n",
    "    offspring2[-1] = ['done', (len(offspring2) - 1)]\n",
    "    candid2 = Candidate(offspring2)\n",
    "    candid2.set_start_state(match_start)\n",
    "    return candid1, candid2\n",
    "\n",
    "def re_execute(model,env,candidate):\n",
    "  obs =env.reset()\n",
    "  obs =env.set_state(deepcopy(candidate.get_start_state()))\n",
    "  episode = candidate.get_candidate_values()\n",
    "  steps_to_mut_point = len(episode)\n",
    "  episode_reward = 0.0\n",
    "  done= False \n",
    "  counter = 0 \n",
    "  for i in range(steps_to_mut_point):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action_selected = episode[i][1]\n",
    "    if action_selected == 'Mut':\n",
    "      # print(episode[i])\n",
    "      # print(episode[i][0])\n",
    "      action_selected, _ = model.predict(episode[i][0], deterministic=True)\n",
    "      # print(\"ddd\",i,\"eee\",steps_to_mut_point)\n",
    "      # print(action_selected)\n",
    "      # break\n",
    "    obs, reward, terminated , truncated, info = env.step(int(action_selected)) # its very important to select the action here it means that we may \n",
    "    counter+=1\n",
    "    #follow the previous path until the mutation point or we follow the route that the trained agent wants to follow forcing vs following \n",
    "    episode_reward += reward\n",
    "    # print(\"counter\",counter)\n",
    "    if terminated or truncated:\n",
    "      break \n",
    "  for j in range(200):\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated ,truncated, info = env.step(action) \n",
    "    counter+=1\n",
    "    episode_reward += reward\n",
    "  assert terminated or truncated\n",
    "  if episode_reward>201:\n",
    "    assert False \n",
    "  return env.info['mem'][-((counter)+1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2fe22",
   "metadata": {},
   "source": [
    "## Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Candidate:\n",
    "    def __init__(self, candidates_vals):\n",
    "        if isinstance(candidates_vals, (np.ndarray, np.generic)):\n",
    "            self.candidate_values = candidates_vals.tolist()\n",
    "        else:\n",
    "            self.candidate_values = candidates_vals\n",
    "        self.objective_values = []\n",
    "        self.objectives_covered = []\n",
    "        self.crowding_distance = 0\n",
    "        self.uncertainity = []\n",
    "        self.start_state = 0\n",
    "        self.information = []\n",
    "        self.mutation = False\n",
    "\n",
    "    def get_candidate_values(self):\n",
    "        return self.candidate_values\n",
    "\n",
    "    def get_uncertainity_value(self, indx):\n",
    "        return self.uncertainity[indx]\n",
    "    def get_uncertainity_values(self):\n",
    "        return self.uncertainity\n",
    "    def set_uncertainity_values(self,uncertain):\n",
    "        self.uncertainity = uncertain\n",
    "    def set_candidate_values(self, cand):\n",
    "        self.candidate_values = cand\n",
    "    def set_candidate_values_at_index(self, indx,val):\n",
    "        self.candidate_values[indx] = val\n",
    "\n",
    "    def get_objective_values(self):\n",
    "        return self.objective_values\n",
    "\n",
    "    def get_objective_value(self, indx):\n",
    "        return self.objective_values[indx]\n",
    "\n",
    "    def set_objective_values(self, obj_vals):\n",
    "        self.objective_values = obj_vals\n",
    "\n",
    "    def add_objectives_covered(self, obj_covered):\n",
    "        if obj_covered not in self.objectives_covered:\n",
    "            self.objectives_covered.append(obj_covered)\n",
    "\n",
    "    def get_covered_objectives(self):\n",
    "        return self.objectives_covered\n",
    "\n",
    "    def set_crowding_distance(self, cd):\n",
    "        self.crowding_distance = cd\n",
    "\n",
    "    def get_crowding_distance(self):\n",
    "        return self.crowding_distance\n",
    "\n",
    "    def exists_in_satisfied(self, indx):\n",
    "        for ind in self.objectives_covered:\n",
    "            if ind == indx:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_objective_covered(self, obj_to_check):\n",
    "        for obj in self.objectives_covered:\n",
    "            if obj == obj_to_check:\n",
    "                return True\n",
    "        return False\n",
    "    def set_start_state(self,start_point):\n",
    "      self.start_state = deepcopy(start_point)\n",
    "\n",
    "    def get_start_state(self):\n",
    "      return self.start_state\n",
    "\n",
    "    def set_info(self, new_information):\n",
    "      self.information.append(new_information)\n",
    "      \n",
    "    def get_info(self):\n",
    "      return self.information\n",
    "\n",
    "    def mutated(self):\n",
    "      self.mutation = True\n",
    "\n",
    "def mutation_number_update(file_address, Mut_Num_to_add, iteration):\n",
    "    if iteration == 0:\n",
    "        with open(file_address, 'wb') as file:\n",
    "            pickle.dump(Mut_Num_to_add, file)\n",
    "        return\n",
    "    with open(file_address, 'rb') as file2:\n",
    "        Mut_num = pickle.load(file2)\n",
    "    print(Mut_num)\n",
    "    if type(Mut_num) == list:\n",
    "        print('list')\n",
    "        buffer = Mut_num\n",
    "        buffer.append(Mut_Num_to_add)\n",
    "        print(buffer)\n",
    "    else:\n",
    "        print('int')\n",
    "        buffer = []\n",
    "        buffer.append(Mut_num)\n",
    "        buffer.append(Mut_Num_to_add)\n",
    "        print(buffer)\n",
    "    with open(file_address, 'wb') as file:\n",
    "        pickle.dump(buffer, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d8e7c9",
   "metadata": {},
   "source": [
    "## MOSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "# domination relation method, same as MOSA\n",
    "def dominates(value_from_pop, value_from_archive, objective_uncovered):\n",
    "    dominates_f1 = False\n",
    "    dominates_f2 = False\n",
    "    for each_objective in objective_uncovered:\n",
    "        f1 = value_from_pop[each_objective]\n",
    "        f2 = value_from_archive[each_objective]\n",
    "        if f1 < f2:\n",
    "            dominates_f1 = True\n",
    "        if f2 < f1:\n",
    "            dominates_f2 = True\n",
    "        if dominates_f1 and dominates_f2:\n",
    "            break\n",
    "    if dominates_f1 == dominates_f2:\n",
    "        return False\n",
    "    elif dominates_f1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def evaulate_population(func, pop, parameters):\n",
    "    for candidate in pop:\n",
    "        if isinstance(candidate, Candidate):\n",
    "            result = func(candidate.get_candidate_values())\n",
    "            candidate.set_objective_values(result)\n",
    "            print(candidate.get_objective_values())\n",
    "\n",
    "\n",
    "def evaulate_population_with_archive(func, pop, already_executed):\n",
    "    to_ret = []\n",
    "    for candidate in pop:\n",
    "        if isinstance(candidate, Candidate):\n",
    "            if candidate.get_candidate_values() in already_executed:\n",
    "                continue\n",
    "\n",
    "            result = func(candidate.get_candidate_values())\n",
    "            candidate.set_objective_values(result)\n",
    "            already_executed.append(candidate.get_candidate_values())\n",
    "            to_ret.append(candidate)\n",
    "    return to_ret\n",
    "\n",
    "\n",
    "def exists_in_archive(archive, index):\n",
    "    for candidate in archive:\n",
    "        if candidate.exists_in_satisfied(index):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_from_archive(obj_index, archive):\n",
    "    for candIndx in range(len(archive)):\n",
    "        candidate = archive[candIndx]\n",
    "        if candidate.exists_in_satisfied(obj_index):\n",
    "            return candidate, candIndx\n",
    "    return None\n",
    "\n",
    "\n",
    "# updating archive with adding the number of objective it satisfies, Same as Mosa paper\n",
    "def update_archive(pop, objective_uncovered, archive, no_of_Objectives, threshold_criteria):\n",
    "    for objective_index in range(no_of_Objectives):\n",
    "        for pop_index in range(len(pop)):\n",
    "            objective_values = pop[pop_index].get_objective_values()\n",
    "            # if not objective_values[objective_index] or not threshold_criteria[objective_index]:\n",
    "            if objective_values[objective_index] <= threshold_criteria[objective_index]:\n",
    "                if exists_in_archive(archive, objective_index):\n",
    "                    archive_value, cand_indx = get_from_archive(objective_index, archive)\n",
    "                    obj_archive_values = archive_value.get_objective_values()\n",
    "                    if obj_archive_values[objective_index] > objective_values[objective_index]:\n",
    "                        value_to_add = pop[pop_index]\n",
    "                        value_to_add.add_objectives_covered(objective_index)\n",
    "                        # archive.append(value_to_add)\n",
    "                        archive[cand_indx] = value_to_add\n",
    "                        if objective_index in objective_uncovered:\n",
    "                            objective_uncovered.remove(objective_index)\n",
    "                        # archive.remove(archive_value)\n",
    "                else:\n",
    "                    value_to_add = pop[pop_index]\n",
    "                    value_to_add.add_objectives_covered(objective_index)\n",
    "                    archive.append(value_to_add)\n",
    "                    if objective_index in objective_uncovered:\n",
    "                        objective_uncovered.remove(objective_index)\n",
    "\n",
    "def select_best(tournament_candidates, objective_uncovered):\n",
    "    best = tournament_candidates[0]  # in case none is dominating other\n",
    "    for i in range(len(tournament_candidates)):\n",
    "        candidate1 = tournament_candidates[i]\n",
    "        for j in range(len(tournament_candidates)):\n",
    "            candidate2 = tournament_candidates[j]\n",
    "            if (dominates(candidate1.get_objective_values(), candidate2.get_objective_values(), objective_uncovered)):\n",
    "                best = candidate1\n",
    "    return best\n",
    "\n",
    "\n",
    "def tournament_selection_improved(pop, size, objective_uncovered):\n",
    "    tournament_candidates = []\n",
    "    for i in range(size):\n",
    "        indx = random.randint(0, len(pop) - 1)\n",
    "        random_candidate = pop[indx]\n",
    "        tournament_candidates.append(random_candidate)\n",
    "\n",
    "    best = select_best(tournament_candidates, objective_uncovered)\n",
    "    return best\n",
    "\n",
    "\n",
    "def tournament_selection(pop, size, objective_uncovered):\n",
    "    tournament_candidates = []\n",
    "    indx = np.random.randint(0, len(pop)-1, size=size)\n",
    "    for i in indx:\n",
    "        tournament_candidates.append(pop[i])\n",
    "\n",
    "    best = select_best(tournament_candidates, objective_uncovered)\n",
    "    return best\n",
    "\n",
    "def generate_offspring_improved(population, model, env, d, objective_uncovered):\n",
    "    population_to_return = []\n",
    "    probability_C = 0.75\n",
    "    probability_M = 0.3\n",
    "    size = len(population)\n",
    "    while (len(population_to_return) < size):\n",
    "        probability_crossover = random.uniform(0, 1)\n",
    "        if probability_crossover <= probability_C:  # 75% probability\n",
    "            off1, off2 = Crossover_improved_v2(population, model, 1, objective_uncovered)\n",
    "            population_to_return.append(off1)\n",
    "            population_to_return.append(off2)\n",
    "        probability_mutation = random.uniform(0, 1)\n",
    "        if probability_mutation <= probability_M:  # 30% probability this in for test purposes\n",
    "            off3 = mutation_improved(population, model, env, objective_uncovered)\n",
    "            population_to_return.append(off3)\n",
    "    return population_to_return\n",
    "\n",
    "\n",
    "def generate_offspring_improved_v2(population,model,env,d,objective_uncovered):\n",
    "    \n",
    "    population_to_return = []\n",
    "    probability_C = 0.75\n",
    "    probability_M = 0.01\n",
    "    size = len(population)\n",
    "    while (len(population_to_return) < size):\n",
    "      probability_crossover = random.uniform(0, 1)\n",
    "      if probability_crossover <= probability_C:  # 75% probability\n",
    "        parent1, parent2 = Crossover_improved_v2(population,model,d,objective_uncovered)\n",
    "        parent1 = mutation_improved_p(parent1, model,env, (1 / len(parent1.get_candidate_values())))\n",
    "        parent2 = mutation_improved_p(parent2, model,env, (1 / len(parent2.get_candidate_values())))\n",
    "        population_to_return.append(parent1)\n",
    "        population_to_return.append(parent2)\n",
    "\n",
    "      if probability_crossover > probability_C:\n",
    "        parent = tournament_selection(population, 10, objective_uncovered) #we may add a very small number of duplicated individulas but its not important as we are removing them in the final executions\n",
    "        population_to_return.append(mutation_improved_p(parent, model,env,(1 / len(parent.get_candidate_values())))) \n",
    "      \n",
    "\n",
    "    return population_to_return\n",
    "\n",
    "def save_all_data(pop,no_of_Objectives,threshold_criteria, stored_data):\n",
    "  '''\n",
    "  This function will save all individulas with objective lower than treshhold \n",
    "\n",
    "  '''\n",
    "  threshold_criteria_to_add_to_archive = [70, 0.06, 0.05, 0.05] \n",
    "  # be careful here ypu can set the satisfiing objectives that based on them you want to store the data  \n",
    "  for individual in pop:\n",
    "    individual_objective = individual.get_objective_values()\n",
    "    for i in range(no_of_Objectives):\n",
    "      if individual_objective[i]<threshold_criteria_to_add_to_archive[i]:\n",
    "        # if individual not in stored_data:\n",
    "        #   ind_ = deepcopy(individual)\n",
    "        #   stored_data.append(ind_)\n",
    "        # individual_objective_values = individual.get_objective_values()\n",
    "        found = False\n",
    "        for j in range(len(stored_data)):\n",
    "          if individual_objective == stored_data[j].get_objective_values():\n",
    "            found = True\n",
    "            break\n",
    "        if not found:\n",
    "          ind_ = deepcopy(individual)\n",
    "          stored_data.append(ind_)\n",
    "  # return stored_data\n",
    "\n",
    "def save_all_data2(pop, stored_data):\n",
    "    '''\n",
    "    This function will save all individulas in generations\n",
    "    you need to remove redundant data (based on fitness and ...)\n",
    "\n",
    "    '''\n",
    "    stored_data.append(list(pop))\n",
    "\n",
    "\n",
    "def Build_Archive(pop, no_of_Objectives, threshold_criteria, stored_data, initial_population):\n",
    "    '''\n",
    "    If you are using the Archive of all generated episodes, this function\n",
    "    removes the duplicated results and builds the Archive.\n",
    "    :param 'pop': current generation\n",
    "    :param 'no_of_Objectives': number of objectives\n",
    "    :param 'threshold_criteria': threshold criteria (we are intrested in episodes that have fitness below these threshold values)\n",
    "    :param 'stored_data': Archive of final episodes (return)\n",
    "    :param 'initial_population': initial population. we are not considering these episodes in our archive for the second senario you need to add the number of faults, (implementation in RQ3)\n",
    "    '''\n",
    "    threshold_criteria_to_add_to_archive = threshold_criteria\n",
    "    # be careful as we can have different values for criterias here to add episodes to archive and for GA stopping criteria\n",
    "    for individual in pop:\n",
    "        individual_objective = individual.get_objective_values()\n",
    "        for i in range(no_of_Objectives):\n",
    "            if individual_objective[i] < threshold_criteria_to_add_to_archive[i]:\n",
    "                found = False\n",
    "                for j in range(len(stored_data)):\n",
    "                    if individual_objective == stored_data[j].get_objective_values():\n",
    "                        found = True\n",
    "                        break\n",
    "                for k in range(len(initial_population)):\n",
    "                    if individual_objective == initial_population[k].get_objective_values():\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    ind_ = deepcopy(individual)\n",
    "                    stored_data.append(ind_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ab18b",
   "metadata": {},
   "source": [
    "## Sorting and RUN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330121a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_dominating_sort(R_T, objective_uncovered):\n",
    "    to_return = []\n",
    "    front = []\n",
    "    count = 0\n",
    "    while len(R_T) > 1:\n",
    "        count = 0\n",
    "        for outer_loop in range(len(R_T)):\n",
    "            best = R_T[outer_loop]\n",
    "            add = True\n",
    "            for inner_loop in range(len(R_T)):\n",
    "                against = R_T[inner_loop]\n",
    "                if best == against:\n",
    "                    continue\n",
    "                if (dominates(best.get_objective_values(), against.get_objective_values(), objective_uncovered)):\n",
    "                    continue\n",
    "                else:\n",
    "                    add = False\n",
    "                    break\n",
    "\n",
    "            if add == True:\n",
    "                if best not in front:\n",
    "                    front.append(best)\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "        if len(front) > 0:\n",
    "            to_return.append(front)\n",
    "            for i in range(len(front)):\n",
    "                R_T.remove(front[i])\n",
    "                front = []\n",
    "\n",
    "        if (len(to_return) == 0) or (count == 0):  # to check if no one dominates no one\n",
    "            to_return.append(R_T)\n",
    "            break\n",
    "\n",
    "    return to_return\n",
    "\n",
    "def sort_based_on_crowding_distance(e):\n",
    "    values = e.get_crowding_distance()\n",
    "    return values\n",
    "\n",
    "\n",
    "def sort_based_on(e):\n",
    "    values = e.get_objective_values()\n",
    "    return values[0]\n",
    "\n",
    "\n",
    "# sorting based on first objective value\n",
    "def sort_worse(pop):\n",
    "    pop.sort(key=sort_based_on, reverse=True)\n",
    "    return pop\n",
    "\n",
    "\n",
    "# preference sort, same as algorithm\n",
    "def preference_sort(R_T, size, objective_uncovered):\n",
    "    to_return = []\n",
    "    for objective_index in objective_uncovered:\n",
    "        min = 100\n",
    "        best = R_T[0]\n",
    "        for index in range(len(R_T)):\n",
    "            objective_values = R_T[index].get_objective_values()\n",
    "            if objective_values[objective_index] < min:\n",
    "                min = objective_values[objective_index]\n",
    "                best = R_T[index]\n",
    "        to_return.append(best)\n",
    "        R_T.remove(best)\n",
    "    if len(R_T) > 0:\n",
    "        E = fast_dominating_sort(R_T, objective_uncovered)\n",
    "        for i in range(len(E)):\n",
    "            to_return.append(E[i])\n",
    "    return to_return\n",
    "\n",
    "def get_array_for_crowding_distance(sorted_front):\n",
    "    list = []\n",
    "    for value in sorted_front:\n",
    "        objective_values = value.get_objective_values()\n",
    "\n",
    "        np_array = numpy.array(objective_values)\n",
    "        list.append(np_array)\n",
    "\n",
    "    np_list = np.array(list)\n",
    "    cd = calc_crowding_distance(np_list)\n",
    "    return cd\n",
    "\n",
    "def assign_crowding_distance_to_each_value(sorted_front, crowding_distance):\n",
    "    for candidate_index in range(len(sorted_front)):\n",
    "        objective_values = sorted_front[candidate_index]\n",
    "        objective_values.set_crowding_distance(crowding_distance[candidate_index])\n",
    "\n",
    "\n",
    "def run_search(func, initial_population, no_of_Objectives, criteria, archive, logger, start, time_budget, size, d, env,\n",
    "               parameters, second_archive, gens):\n",
    "    global MUTATION_NUMBER\n",
    "    MUTATION_NUMBER = 0\n",
    "    threshold_criteria = criteria\n",
    "    objective_uncovered = []\n",
    "    print(\"initial population \", type(initial_population), len(initial_population))\n",
    "\n",
    "    for obj in range(no_of_Objectives):\n",
    "        objective_uncovered.append(obj)  # initializing number of uncovered objective\n",
    "\n",
    "    random_population = initial_population\n",
    "\n",
    "    P_T = copy.copy(random_population)\n",
    "    evaulate_population(func, random_population,\n",
    "                        parameters)  # evaluating whole generation and storing results propabibly its with candidates\n",
    "\n",
    "    #print(random_population[0].get_objective_values())\n",
    "    update_archive(random_population, objective_uncovered, archive, no_of_Objectives,\n",
    "                   threshold_criteria)  # updating archive\n",
    "    # save initial population\n",
    "    save_all_data2(random_population, gens)\n",
    "    iteration = 0\n",
    "    # limit of number of generations\n",
    "    while iteration < 10:\n",
    "        iteration = iteration + 1  # iteration count\n",
    "        # To-DO: limit by the time budget instead of the generation number\n",
    "        for arc in archive:\n",
    "            logger.info(\"***ARCHIVE***\")\n",
    "            logger.info(\"\\nValues: \" + str(\n",
    "                arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
    "                arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
    "                arc.get_covered_objectives()))\n",
    "        print(\"Iteration count: \" + str(iteration))\n",
    "        logger.info(\"Iteration is : \" + str(iteration))\n",
    "        logger.info(\"Number of mutations : \" + str(MUTATION_NUMBER))\n",
    "\n",
    "        R_T = []\n",
    "\n",
    "        Q_T = generate_offspring_improved_v2(P_T, model, env, d,\n",
    "                                             objective_uncovered)  # generate offsprings using crossover and mutation\n",
    "\n",
    "        evaulate_population(func, Q_T, parameters)  # evaluating offspring\n",
    "        update_archive(Q_T, objective_uncovered, archive, no_of_Objectives, threshold_criteria)  # updating archive\n",
    "        save_all_data(Q_T, no_of_Objectives, threshold_criteria, second_archive)\n",
    "        # save generations\n",
    "        save_all_data2(Q_T, gens)\n",
    "        R_T = copy.deepcopy(P_T)  # R_T = P_T union Q_T\n",
    "        R_T.extend(Q_T)\n",
    "\n",
    "        F = preference_sort(R_T, size, objective_uncovered)  # Preference sorting and getting fronts\n",
    "\n",
    "        if len(objective_uncovered) == 0 :  # checking if all objectives are covered\n",
    "            print(\"all_objectives_covered\")\n",
    "            logger.info(\"***Final-ARCHIVE***\")\n",
    "            print((\"***Final-ARCHIVE***\"))\n",
    "            for arc in archive:\n",
    "                print(\"\\nValues: \" + str(\n",
    "                    arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
    "                    arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
    "                    arc.get_covered_objectives()))\n",
    "\n",
    "                logger.info(\"\\nValues: \" + str(\n",
    "                    arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
    "                    arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
    "                    arc.get_covered_objectives()))\n",
    "            logger.info(\"Iteration is : \" + str(iteration))\n",
    "            logger.info(\"Number of mutations : \" + str(MUTATION_NUMBER))\n",
    "            break\n",
    "\n",
    "        P_T_1 = []  # creating next generatint PT+1\n",
    "        index = 0\n",
    "\n",
    "        while len(P_T_1) <= size:  # if length of current generation is less that size of front at top then add it\n",
    "\n",
    "            if not isinstance(F[index], Candidate):\n",
    "                if len(P_T_1) + len(F[index]) > size:\n",
    "                    break\n",
    "            else:\n",
    "                if len(P_T_1) + 1 > size:\n",
    "                    break\n",
    "\n",
    "            front = F[index]\n",
    "            if isinstance(F[index], Candidate):  # if front contains only one item\n",
    "                P_T_1.append(F[index])\n",
    "                F.remove(F[index])\n",
    "            else:\n",
    "                for ind in range(len(F[index])):  # if front have multiple items\n",
    "                    val = F[index][ind]\n",
    "                    P_T_1.append(val)\n",
    "\n",
    "                F.remove(F[index])\n",
    "        while (len(P_T_1)) < size:  # crowding distance\n",
    "            copyFront = copy.deepcopy(F[index])\n",
    "            sorted_front = sort_worse(copyFront)  # sort before crowding distance\n",
    "\n",
    "            crowding_distance = get_array_for_crowding_distance(sorted_front)  # coverting to libaray compaitble array\n",
    "            assign_crowding_distance_to_each_value(sorted_front,\n",
    "                                                   crowding_distance)  # assinging each solution its crowding distance\n",
    "            sorted_front.sort(key=sort_based_on_crowding_distance, reverse=True)  # sorting based on crowding distance\n",
    "\n",
    "            if (len(sorted_front) + len(\n",
    "                    P_T_1)) > size:  # maintaining length and adding solutions with most crowding distances\n",
    "                for sorted_front_indx in range(len(sorted_front)):\n",
    "                    candidate = sorted_front[sorted_front_indx]\n",
    "                    P_T_1.append(candidate)\n",
    "                    if len(P_T_1) >= size:\n",
    "                        break\n",
    "\n",
    "            index = index + 1\n",
    "\n",
    "        P_T_1 = P_T_1[0:size]\n",
    "        P_T = P_T_1  # assigning PT+1 to PT\n",
    "\n",
    "def minimize(func, population, lb, ub, no_of_Objectives, criteria, time_budget, logger, archive, size, d, env,\n",
    "             parameters, second_archive, gens):\n",
    "    assert hasattr(func, '__call__')\n",
    "\n",
    "    start = time.time()\n",
    "    run_search(func, population, no_of_Objectives, criteria, archive, logger, start, time_budget, size, d, env,\n",
    "               parameters, second_archive, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ffddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar_caseStudy():\n",
    "    def __init__(self):\n",
    "        logger = logging.getLogger()\n",
    "        now = datetime.now()\n",
    "        log_file = 'log/STARLA' + str(i) + '_V2' + str(now) + '.log'\n",
    "        logging.basicConfig(filename=log_file,\n",
    "                            format='%(asctime)s %(message)s')\n",
    "        self.parameters = [model,d,unique5]\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    def _evaluate(self,x):\n",
    "        fv = x\n",
    "        model,d,unique5 = self.parameters\n",
    "        obj1 = fitness_reward(fv)\n",
    "        if obj1==None:\n",
    "          debug_data1=[fv,x]\n",
    "          with open(f'/content/drive/MyDrive/debug/data.pickle', 'wb') as file:\n",
    "              pickle.dump(debug_data1, file)\n",
    "          assert False\n",
    "        obj2 = fitness_confidence(fv,model,'m')\n",
    "        binary_fv = translator(fv,model,d,unique5)\n",
    "        obj3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
    "        # obj4 = fitness_functional_probability(RF_RF_1rep,binary_fv)\n",
    "        to_ret = [obj1,obj2,obj3]\n",
    "        logger = logging.getLogger()\n",
    "        logger.info(str(fv)+\",\"+str(to_ret))\n",
    "        return to_ret\n",
    "\n",
    "\n",
    "def run(i,population ,archive ,second_archive, gens):\n",
    "    env=mtc_wrapped\n",
    "    d=500\n",
    "    size = len(population)\n",
    "    lb = [0, 0, 0]\n",
    "    ub = [100000,1000000,100000]\n",
    "\n",
    "    parameters = [model,d,unique1]\n",
    "    threshold_criteria = [-180, 0.04, 0.05]\n",
    "\n",
    "\n",
    "    no_of_Objectives = 3;\n",
    "\n",
    "    now = datetime.now()\n",
    "    global logger\n",
    "    logger = logging.getLogger()\n",
    "    log_file = 'D:\\\\code\\\\RLtest\\\\starla\\\\data\\\\testcase\\\\Results' + str(i) + '_V2' + str(now).replace(\":\",\"_\") + '.log'\n",
    "    logging.basicConfig(filename=log_file,\n",
    "                        format='%(asctime)s %(message)s')\n",
    "\n",
    "    logger.setLevel(logging.WARNING)\n",
    "\n",
    "    archive = minimize(MountainCar_caseStudy()._evaluate, population, lb, ub,\n",
    "                       no_of_Objectives, threshold_criteria, 7200, \n",
    "                       logger,archive,size,d,env , parameters, second_archive,gens)\n",
    "    logger.info(\"Iteration completed\")\n",
    "    logger.info(\"mu\"+str(MUTATION_NUMBER))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3c4a6",
   "metadata": {},
   "source": [
    "## analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d33c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_result(result):\n",
    "    '''\n",
    "    this function is to aggrigate the differences of the results\n",
    "    :param `result`: this is the output of the re-execution-improved function\n",
    "    :return ``:\n",
    "    '''\n",
    "    total_dif = 0\n",
    "    # store_diff=[]\n",
    "    for i in range(len(result)):\n",
    "        dif = abs(result[i][1][0] - result[i][1][1])\n",
    "        # store_diff.append([i,dif])\n",
    "        total_dif += dif\n",
    "    return total_dif  # , store_diff\n",
    "\n",
    "\n",
    "def get_objective_distribution_and_set_candidate_objectives(population, model, d,\n",
    "                                                            unique1, RF_FF_1rep,\n",
    "                                                            RF_RF_1rep):\n",
    "    fit1_list = []\n",
    "    fit2_list = []\n",
    "    fit3_list = []\n",
    "    fit4_list = []\n",
    "    for i in range(len(population)):\n",
    "        ind_data = population[i].get_candidate_values()\n",
    "        fit1 = fitness_reward(ind_data)\n",
    "        fit2 = fitness_confidence(ind_data, model, 'm')\n",
    "        binary_fv = translator(ind_data, model, d, unique1)\n",
    "        fit3 = fitness_functional_probability(RF_FF_1rep, binary_fv)\n",
    "        fit4 = fitness_reward_probability(RF_RF_1rep, binary_fv)\n",
    "        obj = [fit1, fit2, fit3, fit4]\n",
    "        population[i].set_objective_values(obj)\n",
    "        fit1_list.append(fit1)\n",
    "        fit2_list.append(fit2)\n",
    "        fit3_list.append(fit3)\n",
    "        fit4_list.append(fit4)\n",
    "    return fit1_list, fit2_list, fit3_list, fit4_list\n",
    "\n",
    "def was_in_initial_population(solution, population, no_of_Objectives):\n",
    "    flag = False\n",
    "    for individuals_ in population:\n",
    "        if individuals_.get_objective_values() == solution.get_objective_values():\n",
    "            flag = True\n",
    "    if not flag:\n",
    "        return solution\n",
    "    if flag:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def analyze_set_differences(differences_set):\n",
    "    '''\n",
    "    input is a set of differences\n",
    "    '''\n",
    "    analyzed_results = []\n",
    "    for item in differences_set:\n",
    "        res = [len(item[0]), analyze_result(item[0]), item[1], len(item[0]) / item[1]]\n",
    "        analyzed_results.append(res)\n",
    "    return analyzed_results\n",
    "\n",
    "def extract_differences(solution_set):\n",
    "    '''\n",
    "    input is a set of solutions like archive or second_archive\n",
    "    the output a list ([list of differences as a result of re-execution],reward)\n",
    "    '''\n",
    "    differences = []\n",
    "    for dastan in solution_set:\n",
    "        reward = dastan.get_objective_values()[0]\n",
    "        differences.append([re_execution_improved_v2(model, env2, dastan), reward])\n",
    "    return differences\n",
    "\n",
    "\n",
    "def get_results_distribution(results):\n",
    "    num_of_diff = []\n",
    "    diff_confi = []\n",
    "    diff_ration = []\n",
    "    for item in results:\n",
    "        num_of_diff.append(item[0])\n",
    "        diff_confi.append(item[1])\n",
    "        diff_ration.append(item[3])\n",
    "    return num_of_diff, diff_confi, diff_ration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85334de",
   "metadata": {},
   "source": [
    "## mountaincar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58682cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test_2(model, env, Num):\n",
    "    obs, info= env.reset()\n",
    "    counter = 1\n",
    "    episode_reward = 0.0\n",
    "    for i in range(Num):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            counter += 1\n",
    "            end = i\n",
    "            episode_reward = 0.0\n",
    "            obs,info= env.reset()\n",
    "    iter = deepcopy(counter)\n",
    "    u = 1\n",
    "    while iter > 1:\n",
    "        if type(env.info['mem'][-u][0]) is str and env.info['mem'][-u][0] == 'done':\n",
    "            lastpoint = -u\n",
    "            iter -= 1\n",
    "        u += 1\n",
    "    fin = Num - end\n",
    "    start = -Num - counter\n",
    "    randomtest = env.info['mem'][lastpoint:-fin]\n",
    "    ran_state = env.info['state'][(-counter + 1):-1]\n",
    "    return randomtest, ran_state\n",
    "\n",
    "class model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(6, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 64)\n",
    "        self.fc3 = torch.nn.Linear(64 ,64)\n",
    "        self.fc4 = torch.nn.Linear(64 ,32)\n",
    "        self.fc5 = torch.nn.Linear(32 ,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc1(input)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.fc3(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.fc4(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.fc5(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40926c",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29544908",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "Drive_model = \"D:\\\\code\\\\RLtest\\\\starla\\\\data\\\\model\\\\97.zip\"\n",
    "mtc = gym.make('MountainCar-v0')\n",
    "mtc_wrapped = StoreAndTerminateWrapper(mtc)\n",
    "model = DQN('MlpPolicy',env=mtc_wrapped, verbose=1)\n",
    "model = model.load(Drive_model)\n",
    "d = 500\n",
    "\n",
    "MUTATION_NUMBER = 0  # set the mutation counter to 0\n",
    "Run_number = 4\n",
    "for s in range(1, 10):\n",
    "    print(f'\\033[33mepoch s{s} begin\\033[0m')\n",
    "# print('pong')\n",
    "# for d in [1, 0.5, 0.1, 0.05]:\n",
    "    ee, qq = random_test_2(model,mtc_wrapped, 1500_000)\n",
    "    test, teststate = fix_testing(ee, qq,mtc_wrapped)\n",
    "    print(f'Abstract Level: {d}')\n",
    "    unique1, uni1 = Abstract_classes(test, d, model)\n",
    "    unique5 = unique1\n",
    "    hash_table = {}\n",
    "    for k, val in enumerate(unique1):\n",
    "        hash_table[val] = k\n",
    "\n",
    "    epsilon = 0.1\n",
    "    data1_x_b, data1_y_b, data1_y_f_b = ML_first_representation_func_based(d,\n",
    "                                                                       is_functional_fault,\n",
    "                                                                       is_reward_fault\n",
    "                                                                       ,model\n",
    "                                                                       ,test\n",
    "                                                                       ,unique1)\n",
    "\n",
    "#########################################################  Train ML -  Reward fault predictor  #############\n",
    "\n",
    "    X_train_reward_fault, X_test_reward_fault, y_train_reward_fault, y_test_reward_fault = train_test_split(data1_x_b, data1_y_b, test_size=0.33, random_state=42)\n",
    "\n",
    "    RF_RF_1rep = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "    RF_RF_1rep.fit(X_train_reward_fault,y_train_reward_fault)\n",
    "#report(RF_RF_1rep,X_train_reward_fault,y_train_reward_fault,X_test_reward_fault,y_test_reward_fault)\n",
    "\n",
    "#########################################################  Train ML - Functional fault predictor #############\n",
    "\n",
    "\n",
    "    X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(data1_x_b, data1_y_f_b, test_size=0.33, random_state=42)\n",
    "    RF_FF_1rep = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "    RF_FF_1rep.fit(X_train_f,y_train_f)\n",
    "    \n",
    "    print('len population', len(test))\n",
    "    start_state_ep1 = teststate\n",
    "    ep1 = test\n",
    "    population = []\n",
    "    for i in range(0, 1500):  # size of the initial population is 1500\n",
    "        #if len(ep1[i]) < 400:\n",
    "            #continue\n",
    "        cd = Candidate(ep1[i])\n",
    "        cd.set_start_state(start_state_ep1[i])\n",
    "        population.append(cd)\n",
    "    archive1 = []\n",
    "    second_arch1 = []\n",
    "    generations = []  # all of the episodes generated during the search\n",
    "    run(0, population, archive1, second_arch1, generations)\n",
    "    with open(\n",
    "            f'D:\\\\code\\\\Results\\\\May17_arch1_r110_rt400_population1500lastfull_run{Run_number}_{s}.pickle',\n",
    "            'wb') as file:\n",
    "        pickle.dump(archive1, file)\n",
    "    with open(\n",
    "           f'D:\\\\code\\\\Results\\\\May17_second_arch1_r110_rt400_population1500lastfull_run{Run_number}_{s}.pickle',\n",
    "            'wb') as file:\n",
    "        pickle.dump(second_arch1, file)\n",
    "    with open(\n",
    "             f'D:\\\\code\\\\Results\\\\May17_generations_r110_rt400_population1500lastfull_run{Run_number}_{s}.pickle',\n",
    "            'wb') as file:\n",
    "        pickle.dump(generations, file)\n",
    "    mutation_number_update( f'D:\\\\code\\\\Results\\\\Mutation_number_run{Run_number}.pickle', MUTATION_NUMBER,\n",
    "                           s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdpfuzz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
