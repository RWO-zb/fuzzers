{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f103a2",
   "metadata": {},
   "source": [
    "##  Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4be63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import math\n",
    "from random import seed\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import impute\n",
    "import statistics\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import ceil\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import jaccard_score\n",
    "import time\n",
    "import multiprocessing\n",
    "from pymoo.algorithms.moo.nsga2 import calc_crowding_distance\n",
    "import subprocess\n",
    "import logging\n",
    "from csv import reader\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae02c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon: 30\n"
     ]
    }
   ],
   "source": [
    "if any(arg.startswith('--f=') for arg in sys.argv):\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epsilon\", type=int, default=30)\n",
    "parser.add_argument(\"--abstract-level\", type=float, default=1)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f'epsilon: {args.epsilon}')\n",
    "DISPLAY_SCREEN = False\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "DD = args.abstract_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382437",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreAndTerminateWrapper(gym.Wrapper):\n",
    "    '''\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super(StoreAndTerminateWrapper,self).__init__(env)\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.env=env\n",
    "        self.mem = []\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_state = None\n",
    "        self.first_obs = 0\n",
    "        self.prev_obs = None\n",
    "        self.states_list = []\n",
    "        self.info = {}\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.current_step = 0\n",
    "        obs, info = self.env.reset(*args, **kwargs)\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_obs = obs\n",
    "        return obs,info\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step == 0:\n",
    "            self.prev_obs = self.first_obs\n",
    "            self.first_state = deepcopy(self.env)\n",
    "            self.states_list.append(self.first_state)\n",
    "        self.current_step += 1\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.TotalReward += reward\n",
    "        self.mem.append(tuple((self.prev_obs,action)))\n",
    "        self.prev_obs = obs\n",
    "        if self.current_step >= self.max_steps:\n",
    "          truncated = True\n",
    "        if obs[0] <= -1.2:\n",
    "          truncated = True\n",
    "          reward = -201 - self.TotalReward\n",
    "          self.TotalReward = -200\n",
    "        if terminated or truncated:\n",
    "          self.mem.append(tuple(('done',self.TotalReward)))\n",
    "        self.info['mem'] = self.mem\n",
    "        self.info['state'] = self.states_list\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.env = deepcopy(state)\n",
    "        obs = np.array(list(self.env.unwrapped.state))\n",
    "        self.current_step = 0\n",
    "        self.TotalReward = 0.0\n",
    "        self.first_obs = obs\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fba8e",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4736946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel():\n",
    "    def __init__(self, torch_net: torch.nn.Module):\n",
    "        self.torch_net = torch_net\n",
    "\n",
    "    def abstract_state(self, state1, d):\n",
    "        if type(state1) == str:\n",
    "            if state1 == 'done':\n",
    "                return 'end'\n",
    "        state1 = torch.tensor(np.array(state1), dtype=torch.float32, requires_grad=False).cuda()\n",
    "        q_value = self.torch_net(state1).cpu().detach().numpy()\n",
    "        if q_value.ndim == 1:\n",
    "            return tuple(np.ceil(q_value / d))\n",
    "        else:\n",
    "            return [tuple(i) for i in np.ceil(q_value / d)]\n",
    "\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, requires_grad=False).cuda()\n",
    "        q_value = self.torch_net(obs).cpu().detach().numpy()\n",
    "        if deterministic:\n",
    "            return np.argmax(q_value)\n",
    "        else:\n",
    "            return np.random.choice([0, 1], p=q_value / q_value.sum())\n",
    "\n",
    "    def action_probability(self, state):\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32, requires_grad=False).cuda()\n",
    "        q_value = self.torch_net(state).cpu().detach().numpy()\n",
    "        if q_value.ndim == 1:\n",
    "            return q_value / q_value.sum()\n",
    "        else:\n",
    "            div_factor = q_value.sum(axis=1)\n",
    "            return (q_value.T / div_factor).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874c7fa",
   "metadata": {},
   "source": [
    "## RL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "047f256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportional_sampling_whitout_replacement(index, size):\n",
    "    s = 0\n",
    "    s = sum(np.array(index))\n",
    "    p = [ind / s for ind in index]\n",
    "    samples = np.random.choice(index, size=size, replace=False, p=p)\n",
    "    return samples\n",
    "\n",
    "def population_sample(episodes , ind,  pop_size , threshold, functional_fault_size, reward_fault_size):\n",
    "  \"\"\"\n",
    "  This function is meant to sample episodes from training after that you need to add test episodes using random_test \n",
    "  Set the parameters as you want but be careful the input episodes for this function is the memory of the agent and each step has seperate index \n",
    "  this function returns the final steps of the selected function then you need to extract that episodes from the input memore that is called 'episodes'\n",
    "  use the episodes extract function ... \n",
    "\n",
    "  samples n episodes from training n1 functinal faults and n2 reward faults \n",
    "  reward faults are episodes with reward bellow the thresthreshold \n",
    "  from random test samples M episodes m1 random episode and\n",
    "  m2 episodes with sudden reward change we dont have a sudden reward change in this example  \n",
    "  \"\"\"\n",
    "  epsilon = 0.1\n",
    "  index = []\n",
    "  functional_fault = []\n",
    "  reward_fault = []\n",
    "  start_states =[]\n",
    "  ind  = np.where(np.array(episodes)==('done',))\n",
    "  index= ind[0]\n",
    "  print(len(ind[0]),'episodes from training')\n",
    "  population=[]\n",
    "  for i in index:\n",
    "    _,r = episodes[i]\n",
    "    if abs(episodes[i-1][0][0])<(mtc_wrapped.low[0]+epsilon):\n",
    "      functional_fault.append(i)\n",
    "      print('function fault') \n",
    "    if r<threshold:\n",
    "      reward_fault.append(i)\n",
    "      print('reward fault')\n",
    "  if len(functional_fault)<functional_fault_size:\n",
    "    print('functional faults size is' ,len(functional_fault),' and its less than desired number' )\n",
    "    population += functional_fault\n",
    "    print('sampling more random episodes instead ...!')\n",
    "  if len(functional_fault)==functional_fault_size:\n",
    "    population += functional_fault\n",
    "  if len(functional_fault)>functional_fault_size:\n",
    "    # proportianl_sample_whitout_replacement()\n",
    "    sam1=proportional_sampling_whitout_replacement(functional_fault,functional_fault_size)\n",
    "    print(population)\n",
    "    print(\"ff\",len(functional_fault))\n",
    "    population += sam1\n",
    "  if len(reward_fault)<reward_fault_size:\n",
    "    print('reward faults size is' ,len(reward_fault),' and its less than desired number' )\n",
    "    population += reward_fault\n",
    "    print('sampling more random episodes instead ...!')\n",
    "  if len(reward_fault)==reward_fault_size:\n",
    "    population += reward_fault\n",
    "  if len(reward_fault)>reward_fault_size:\n",
    "    #proportional sampling\n",
    "    sam2 = proportional_sampling_whitout_replacement(reward_fault,reward_fault_size)\n",
    "    population += list(sam2)\n",
    "  r_size= pop_size-len(population)\n",
    "  # random_test(model,env,r_size)\n",
    "  print(\"RF\",len(reward_fault))\n",
    "  # population += reward_fault\n",
    "  return population , r_size\n",
    "\n",
    "def episode_extract(sampled_index, episodes):\n",
    "  epis = []\n",
    "  for i in sampled_index:\n",
    "    # print(episodes[i])\n",
    "    j = i-1\n",
    "    while not episodes[j][0] == 'done':\n",
    "      # print(episodes[j])\n",
    "      if j==0:\n",
    "        break\n",
    "      j-=1\n",
    "    slice1 = episodes[(j+1):(i+1)]\n",
    "    epis.append(slice1)\n",
    "    assert len(slice1)>0, 'Attempt to return Empty episode'\n",
    "  return epis\n",
    "\n",
    "def fitness_reward(episode):\n",
    "    \"\"\"\n",
    "    here the reward could be calculated as the lengh of the episode; Since the\n",
    "    reward of the cartpole is defined based on the number of steps without falling\n",
    "    last part of the episode contains the signal of ('done',reward)\n",
    "    \"\"\"\n",
    "    return len(episode) - 1\n",
    "\n",
    "def fitness_confidence(episode, model, mode):\n",
    "    \"\"\"\n",
    "    confidence level is define as differences between the highest and\n",
    "    second highest action probabilities of selecting actions OR\n",
    "    the ratio between the highest and lowest/second highest action probability\n",
    "    :param `mode`: r for ration and m for differences\n",
    "    :param `model`: is the RL agent\n",
    "    :param `episode`: is the episode values or sequence from the rl\n",
    "    \"\"\"\n",
    "    cl = 0.0\n",
    "    if type(episode[-1][0]) is not str or episode[-1][0] != 'done':\n",
    "        assert False, \"last state is not string 'done'\"\n",
    "\n",
    "    state_list = [k[0] for k in episode[:-1]]\n",
    "    prob = model.action_probability(state_list)\n",
    "    prob.sort(axis=1)\n",
    "    if mode == 'm':\n",
    "        return (prob[:, -1] - prob[:, -2]).sum() / episode[-1][1]\n",
    "    elif mode == 'r':\n",
    "        return (prob[:, -1] / prob[:, -2]).sum() / episode[-1][1]\n",
    "    print(\"WARNING nothing returned\", episode)\n",
    "\n",
    "def fitness_reward_probability(ml, binary_episode):\n",
    "    \"\"\"\n",
    "    This function returns the third fitness funciton that is ment to guide the search toward\n",
    "    the episodes with a higher probability of a reward fault and as we have a minimizing\n",
    "    optimization funciton in MOSA we neeed to change this functionwe can either go with the\n",
    "    negation of the probability of the reward fault = 1-probability of the reward fault\n",
    "    that is equal to the probability of the bein a non-faulty episode\n",
    "    :param `ml`: RF_FF_1rep for functional fault\n",
    "    :param `binary episode`: episodes decodeed as having abstract states\n",
    "    \"\"\"\n",
    "    # return -(ml.predict_proba(episode)[0][1])\n",
    "    return ml.predict_proba(binary_episode)[0][0]\n",
    "\n",
    "def fitness_functional_probability(ml, binary_episode):\n",
    "    return ml.predict_proba(binary_episode)[0][0]\n",
    "\n",
    "def state_abstraction(model, state1, state2, d):\n",
    "    \"\"\"\n",
    "    This function compares to state, if they were in the same abstract class\n",
    "    function returs 'True' otherwise 'False'\n",
    "    \"\"\"\n",
    "    q_value1 = model.step_model.step([state1])\n",
    "    q_value2 = model.step_model.step([state2])\n",
    "    for i in range(len(q_value1[1][0])):\n",
    "        print(q_value1[1][0][i])\n",
    "        print(q_value2[1][0][i])\n",
    "        if ceil(q_value1[1][0][i] / d) == ceil(q_value2[1][0][i] / d):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def report(model2, x_train, y_train, x_test, y_test):\n",
    "    plt.ion()\n",
    "    print(\"********************** reporting the result of the model **************************\")\n",
    "    print('The score for train data is {0}'.format(model2.score(x_train, y_train)))\n",
    "    print('The score for test data is {0}'.format(model2.score(x_test, y_test)))\n",
    "\n",
    "    predictions_train = model2.predict(x_train)\n",
    "    predictions_test = model2.predict(x_test)\n",
    "\n",
    "    print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
    "\n",
    "    print(\n",
    "        'the test recall for the class yes is {0}'.format(metrics.recall_score(y_test, predictions_test, pos_label=1)))\n",
    "    print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test, predictions_test, pos_label=0)))\n",
    "\n",
    "    print('the training recall for the class yes is {0}'.format(\n",
    "        metrics.recall_score(y_train, predictions_train, pos_label=1)))\n",
    "    print('the training recall for the class no is {0}'.format(\n",
    "        metrics.recall_score(y_train, predictions_train, pos_label=0)))\n",
    "\n",
    "    print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
    "\n",
    "    print('the test precision for the class yes is {0}'.format(\n",
    "        metrics.precision_score(y_test, predictions_test, pos_label=1)))\n",
    "    print('the test precision for the class no is {0}'.format(\n",
    "        metrics.precision_score(y_test, predictions_test, pos_label=0)))\n",
    "\n",
    "    print('the training precision for the class yes is {0}'.format(\n",
    "        metrics.precision_score(y_train, predictions_train, pos_label=1)))\n",
    "    print('the training precision for the class no is {0}'.format(\n",
    "        metrics.precision_score(y_train, predictions_train, pos_label=0)))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(classification_report(y_test, predictions_test, target_names=['NO ', 'yes']))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"\\n\\nspecifity :\", specificity)\n",
    "    print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
    "    CM = metrics.confusion_matrix(y_test, predictions_test)\n",
    "    print(\"The confusion Matrix:\")\n",
    "    print(CM)\n",
    "    print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
    "    print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
    "    ConfusionMatrixDisplay(CM, display_labels=model2.classes_).plot()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')\n",
    "    display.plot()\n",
    "    plt.pause(3)\n",
    "    plt.ioff()\n",
    "\n",
    "def fix_testing(testing_episodes, testing_states, Env2):\n",
    "    buffer = []\n",
    "    episodes_set = []\n",
    "    j = 0\n",
    "    for i in range(len(testing_episodes)):\n",
    "        if type(testing_episodes[i][0]) is str and testing_episodes[i][0] == 'done':\n",
    "            if i == 0:\n",
    "                continue\n",
    "            buffer.append(testing_episodes[i])\n",
    "            episodes_set.append(buffer)\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(testing_episodes[i])\n",
    "    if not (episodes_set[0][0][0] == Env2.set_state(testing_states[0])).all():\n",
    "        del testing_states[0]\n",
    "    if not (episodes_set[0][0][0] == Env2.set_state(testing_states[0])).all():\n",
    "        assert False, 'problem in starting states'\n",
    "    if len(episodes_set) != len(testing_states):\n",
    "        del testing_states[-1]\n",
    "    if len(episodes_set) != len(testing_states):\n",
    "        assert False, 'problem in data prepration'\n",
    "    return episodes_set, testing_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66581513",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Abstract_classes(ep, abstraction_d, model):\n",
    "    d = abstraction_d\n",
    "    abs_states1 = []\n",
    "    for episode in ep:\n",
    "        for state, action in episode:\n",
    "            if type(state) is str:\n",
    "                continue\n",
    "            abs_states1.append(state)\n",
    "    abs_states1 = model.abstract_state(abs_states1, d)\n",
    "    unique1 = list(set(abs_states1))\n",
    "    uni1 = np.array(unique1)\n",
    "    a = len(abs_states1)\n",
    "    b = len(set(abs_states1))\n",
    "    print(\"abstract states:\", b)\n",
    "    print(\"Concrete states\", a)\n",
    "    print(\"ratio\", b / a)\n",
    "    return unique1, uni1\n",
    "#need modify\n",
    "def ML_first_representation(Abs_d, epsilon_functional_fault_boarder, uni1, model, ep, unique1):\n",
    "    d = Abs_d\n",
    "    epsilon = epsilon_functional_fault_boarder\n",
    "    data1_x_b = []\n",
    "    data1_y_b = []\n",
    "    data1_y_f_b = []\n",
    "    reward_fault_threshold = -180\n",
    "    for episode in ep:\n",
    "        record = np.zeros(len(uni1))\n",
    "\n",
    "        if episode[-1][1] >= reward_fault_threshold:\n",
    "            data1_y_b.append(0)\n",
    "        else:\n",
    "            data1_y_b.append(1)\n",
    "\n",
    "        state_list = [k[0] for k in episode[:-1]]\n",
    "        if is_fail_state(state_list):\n",
    "            data1_y_f_b.append(1)\n",
    "        else:\n",
    "            data1_y_f_b.append(0)\n",
    "\n",
    "        ab = model.abstract_state(state_list, d)\n",
    "        for i in ab:\n",
    "            try:\n",
    "                record[hash_table[i]] = 1\n",
    "            except:\n",
    "                continue\n",
    "        data1_x_b.append(record)\n",
    "\n",
    "    return data1_x_b, data1_y_b, data1_y_f_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e588b9",
   "metadata": {},
   "source": [
    "## Genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(episode, model, d, unique5):\n",
    "    \"\"\"\n",
    "    thid function takes the concrete episodes and returns the encoded episodes\n",
    "    based on the presence and absence of the individuals\n",
    "    :param 'episode': input episode\n",
    "    :param 'model': RL model\n",
    "    :param 'd': abstraction level = 1\n",
    "    :param 'unique5': abstract classes\n",
    "    :return: encoded episodse based on the presence and absence\n",
    "\n",
    "    \"\"\"\n",
    "    d = d\n",
    "    record = np.zeros(len(unique5))\n",
    "    state_list = []\n",
    "    for state, action in episode:\n",
    "        if type(state) is str and state == 'done':\n",
    "            continue\n",
    "        state_list.append(state)\n",
    "\n",
    "    abstract_states = model.abstract_state(state_list, d)\n",
    "    for ab in abstract_states:\n",
    "        try:\n",
    "            record[hash_table[ab]] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return [record]\n",
    "\n",
    "def transform(state):\n",
    "  position = state[0]\n",
    "  noise = np.random.uniform(low=0.95, high=1.05)\n",
    "  new_position= position * noise \n",
    "  new_state =deepcopy(state)\n",
    "  new_state[0] = new_position \n",
    "  return new_state\n",
    "\n",
    "def mutation_improved(population, model, env, objective_uncovered):\n",
    "    \"\"\"\n",
    "    This is the final mutation function\n",
    "    It takes the population as input and returns the mutated individual\n",
    "    :param 'population': Population that we want to mutate\n",
    "    :param 'model': RL model\n",
    "    :param 'env': RL environment\n",
    "    :param 'objective_uncovered: uncovered ubjectives for tournament selection\n",
    "    :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
    "    To-do:\n",
    "    move deepcopy to the cadidate class methods .set info\n",
    "    \"\"\"\n",
    "    parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "    parent1 = deepcopy(parent.get_candidate_values())\n",
    "    if len(parent1) < 3:\n",
    "        assert False, \"parent in mutation is shorter than 3\"\n",
    "    Mutpoint = random.randint(3, (len(parent1) - 3))\n",
    "    new_state = transform(parent1[Mutpoint][0])\n",
    "    action = model.predict(new_state)\n",
    "    if action != int(parent1[Mutpoint][1]):\n",
    "        print('Mutation lured the agent ... ')\n",
    "    new_parent = parent1[:Mutpoint]\n",
    "    new_parent.append([new_state, 'Mut'])\n",
    "    new_cand = Candidate(new_parent)\n",
    "    new_cand.set_start_state(parent.get_start_state())\n",
    "\n",
    "    re_executed_epis = re_execute(model, env, new_cand)\n",
    "\n",
    "    re_executed_cand = Candidate(re_executed_epis)\n",
    "    re_executed_cand.set_start_state(new_cand.get_start_state())\n",
    "    re_executed_cand.set_info(deepcopy(parent.get_info()))\n",
    "    re_executed_cand.set_info([\"mutation is done! \", \"mutpoint was:\", Mutpoint])\n",
    "\n",
    "    return re_executed_cand\n",
    "\n",
    "def mutation_improved_p(parent, model, env, m_rate):\n",
    "    \"\"\"\n",
    "    This is the final mutation function with input of a parent considering internal m_rate\n",
    "    Here we give the parent to themutation funcion based on the given mutation\n",
    "    rate of m_rate, we may mutate the episodes.\n",
    "    :param 'parent' : individual that we want to mutate\n",
    "    :param 'model': RL model\n",
    "    :param 'env': RL environment\n",
    "    :param 'm_rate': mutation : recommended value is 1/len(parent)\n",
    "    :return : mutated individual\n",
    "    To-do:\n",
    "    move deepcopy to the cadidate .set info\n",
    "    \"\"\"\n",
    "    # parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "    global MUTATION_NUMBER\n",
    "    chance = random.uniform(0, 1)\n",
    "    if chance > m_rate:\n",
    "        return parent\n",
    "    else:\n",
    "        parent1 = deepcopy(parent.get_candidate_values())\n",
    "        if len(parent1) < 3:\n",
    "            assert False, \"parent in mutation is shorter than 3\"\n",
    "        Mutpoint = random.randint(1, (len(parent1) - 3))\n",
    "        new_state = transform(parent1[Mutpoint][0])\n",
    "        action = model.predict(new_state)\n",
    "        if action != int(parent1[Mutpoint][1]):\n",
    "            print('Mutation lured the agent ... ')\n",
    "        new_parent = parent1[:Mutpoint]\n",
    "        new_parent.append([new_state, 'Mut'])\n",
    "        new_cand = Candidate(new_parent)\n",
    "        new_cand.set_start_state(parent.get_start_state())\n",
    "        re_executed_epis = re_execute(model, env, new_cand)\n",
    "        re_executed_cand = Candidate(re_executed_epis)\n",
    "        re_executed_cand.set_start_state(new_cand.get_start_state())\n",
    "        re_executed_cand.set_info(deepcopy(parent.get_info()))\n",
    "        re_executed_cand.set_info([\"mutation is done! \", \"mutpoint was:\", Mutpoint])\n",
    "        MUTATION_NUMBER += 1\n",
    "        return re_executed_cand\n",
    "#need modify    \n",
    "def Crossover_improved_v2(population, model, d, objective_uncovered):\n",
    "    \"\"\"\n",
    "    This is the crossover function that we are using\n",
    "    It takes the population as input and returns the mutated individual\n",
    "    :param 'population': Population. we select a parent based on the tournament\n",
    "     selection and then select the mutation point and then search for the matching point.\n",
    "    :param 'model': RL model\n",
    "    :param 'env': RL environment\n",
    "    :param 'objective_uncovered: uncovered ubjectives for tournament selection\n",
    "    :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
    "    To-do:\n",
    "    finding matching episode could be improved bu storing a mapping between concrete states and\n",
    "    \"\"\"\n",
    "    found_match = False\n",
    "    while not (found_match):\n",
    "        parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
    "        parent1 = deepcopy(parent.get_candidate_values())\n",
    "        parent1_start_point = deepcopy(parent.get_start_state())\n",
    "        if len(parent1) < 6:\n",
    "            continue\n",
    "            # assert False, 'input of crossover is shorter than expected '\n",
    "        crosspoint = random.randint(2, (len(parent1) - 3))\n",
    "        abs_class = model.abstract_state(parent1[crosspoint][0], d)\n",
    "        for i in range(50):\n",
    "            indx = random.randint(0, len(population) - 1)\n",
    "            random_candidate = deepcopy(population[indx])\n",
    "            random_cand_data = random_candidate.get_candidate_values()\n",
    "            if len(random_cand_data) < 8:\n",
    "                continue\n",
    "            random_cand_start_point = random_candidate.get_start_state()\n",
    "            random_cand_state_list = [k[0] for k in random_cand_data[2:len(random_cand_data)-3]]\n",
    "            random_ab = model.abstract_state(random_cand_state_list, d)\n",
    "            judge = (np.array(random_ab) == np.array(abs_class)).all(axis=1)\n",
    "            if judge.any():\n",
    "                matches_list = np.where(judge)[0]\n",
    "                found_match = True\n",
    "            if found_match:\n",
    "                break\n",
    "                # print('Crossover. attemp',i)\n",
    "    index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
    "    matchpoint = matches_list[index_match_in_matchlist]\n",
    "    match_candidate = deepcopy(random_candidate)\n",
    "    match = deepcopy(random_cand_data)\n",
    "    match_start = deepcopy(random_cand_start_point)\n",
    "    offspring1 = deepcopy(parent1[:crosspoint])\n",
    "    offspring1 += deepcopy(match[matchpoint:])\n",
    "    offspring1[-1] = ['done', (len(offspring1) - 1)]\n",
    "    candid1 = Candidate(offspring1)\n",
    "    candid1.set_start_state(parent1_start_point)\n",
    "    candid1.set_info(deepcopy(parent.get_info()))\n",
    "    candid1.set_info([\"crossover is Done!\", \"the crossover point is:\", crosspoint])\n",
    "    offspring2 = deepcopy(match[:matchpoint])\n",
    "    offspring2 += deepcopy(parent1[crosspoint:])\n",
    "    offspring2[-1] = ['done', (len(offspring2) - 1)]\n",
    "    candid2 = Candidate(offspring2)\n",
    "    candid2.set_start_state(match_start)\n",
    "    candid2.set_info(deepcopy(match_candidate.get_info()))\n",
    "    candid2.set_info([\"crossover is Done!\", \"the crossover point is:\", matchpoint])\n",
    "\n",
    "    if len(offspring1) < 3:\n",
    "        print(offspring1)\n",
    "        assert False, 'created offspring 1 in crossover is shorter than expected '\n",
    "\n",
    "    if len(offspring2) < 3:\n",
    "        print(offspring2)\n",
    "        assert False, 'created offspring 2 in crossover is shorter than expected '\n",
    "\n",
    "    return candid1, candid2\n",
    "#need to modify\n",
    "def Crossover_improved_v2_random(population, model, d, objective_uncovered):\n",
    "    found_match = False\n",
    "    while not found_match:\n",
    "        i = random.randint(0, len(population))\n",
    "        parent1 = deepcopy(population[i].get_candidate_values())\n",
    "        parent1_start_point = deepcopy(population[i].get_start_state())\n",
    "        matches_list = []\n",
    "        crosspoint = random.randint(1, (len(parent1) - 3))\n",
    "        abs_class = list(model.abstract_state(parent1[crosspoint][0], d))\n",
    "        attemp = 0\n",
    "        for i in range(700):\n",
    "            attemp += 1\n",
    "            indx = random.randint(0, len(population) - 1)\n",
    "            random_candidate = deepcopy(population[indx])\n",
    "            random_cand_data = random_candidate.get_candidate_values()\n",
    "            random_cand_start_point = random_candidate.get_start_state()\n",
    "            for st_index in range(1, len(random_cand_data) - 3):\n",
    "                random_ab = list(model.abstract_state(random_cand_data[st_index][0], d))\n",
    "                if random_ab == abs_class:\n",
    "                    matches_list.append(st_index)\n",
    "                    found_match = True\n",
    "            if found_match:\n",
    "                break\n",
    "    print(\"match found in --- attemps\", attemp)\n",
    "    index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
    "    matchpoint = matches_list[index_match_in_matchlist]\n",
    "    match_candidate = random_candidate\n",
    "    match = random_cand_data\n",
    "    match_start = deepcopy(random_cand_start_point)\n",
    "    offspring1 = deepcopy(parent1[:crosspoint])\n",
    "    offspring1 += deepcopy(match[matchpoint:])\n",
    "    offspring1[-1] = ['done', (len(offspring1) - 1)]\n",
    "    candid1 = Candidate(offspring1)\n",
    "    candid1.set_start_state(parent1_start_point)\n",
    "\n",
    "    offspring2 = deepcopy(match[:matchpoint])\n",
    "    offspring2 += deepcopy(parent1[crosspoint:])\n",
    "    offspring2[-1] = ['done', (len(offspring2) - 1)]\n",
    "    candid2 = Candidate(offspring2)\n",
    "    candid2.set_start_state(match_start)\n",
    "    return candid1, candid2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdpfuzz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
